{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imbalanced-learn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route Type</th>\n",
       "      <th>Collision Type</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Surface Condition</th>\n",
       "      <th>Light</th>\n",
       "      <th>Traffic Control</th>\n",
       "      <th>Driver Substance Abuse</th>\n",
       "      <th>Driver At Fault</th>\n",
       "      <th>Injury Severity</th>\n",
       "      <th>Driver Distracted By</th>\n",
       "      <th>Speed Limit</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Time of Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>County</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>DETECTED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>15-25</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>County</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>CLOUDY</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Minor Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>15-25</td>\n",
       "      <td>Monday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Municipality</td>\n",
       "      <td>SAME DIR REAR END</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAWN</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>County</td>\n",
       "      <td>SINGLE VEHICLE</td>\n",
       "      <td>CLOUDY</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>County</td>\n",
       "      <td>SINGLE VEHICLE</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DARK LIGHTS ON</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>dawn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Route Type     Collision Type Weather Surface Condition           Light  \\\n",
       "0        County              OTHER   CLEAR               DRY        DAYLIGHT   \n",
       "1        County              OTHER  CLOUDY               DRY        DAYLIGHT   \n",
       "2  Municipality  SAME DIR REAR END   CLEAR               DRY            DAWN   \n",
       "3        County     SINGLE VEHICLE  CLOUDY               DRY        DAYLIGHT   \n",
       "4        County     SINGLE VEHICLE   CLEAR               DRY  DARK LIGHTS ON   \n",
       "\n",
       "  Traffic Control Driver Substance Abuse Driver At Fault Injury Severity  \\\n",
       "0     NO CONTROLS               DETECTED             Yes       No Injury   \n",
       "1     NO CONTROLS          NONE DETECTED             Yes    Minor Injury   \n",
       "2  TRAFFIC SIGNAL          NONE DETECTED              No       No Injury   \n",
       "3     NO CONTROLS          NONE DETECTED              No       No Injury   \n",
       "4     NO CONTROLS               DETECTED              No       No Injury   \n",
       "\n",
       "  Driver Distracted By Speed Limit Day of Week Time of Day  \n",
       "0       NOT DISTRACTED       15-25      Sunday   afternoon  \n",
       "1       NOT DISTRACTED       15-25      Monday     morning  \n",
       "2       NOT DISTRACTED       30-40     Tuesday     morning  \n",
       "3       NOT DISTRACTED       30-40     Tuesday     morning  \n",
       "4       NOT DISTRACTED       30-40    Thursday        dawn  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/processed/cleaned_data.csv\",dtype=\"category\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlike PCA,Random Forest can be utilized for categorical data feature selection in classification.\n",
    "# Random forest can be used before or after conducting a classification algorithm. \n",
    " \n",
    "# Before conducting a classification algorithm: Feature importance can help you with feature selection \n",
    "\n",
    "# After conducting a classification algorithm: you can examine the feature importance to gain insights into \n",
    "# which features had the most influence on the model's predictions. This retrospective analysis can help you\n",
    "# understand the key factors driving the classification outcomes and interpret the model's behavior.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "df_numeric = df.apply(lambda x: x.cat.codes)\n",
    "y_numeric=df_numeric[\"Injury Severity\"]\n",
    "x_numeric=df_numeric.drop([\"Injury Severity\"],axis=1)\n",
    "rf.fit(x_numeric, y_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1: Day of Week (0.24150974858501528)\n",
      "Feature 2: Collision Type (0.1696195607792837)\n",
      "Feature 3: Route Type (0.09373584935280219)\n",
      "Feature 4: Time of Day (0.0869449529879933)\n",
      "Feature 5: Light (0.07894559692671745)\n",
      "Feature 6: Traffic Control (0.07730874596578606)\n",
      "Feature 7: Weather (0.07452115068470318)\n",
      "Feature 8: Speed Limit (0.051921275845526686)\n",
      "Feature 9: Surface Condition (0.036270814441793454)\n",
      "Feature 10: Driver Substance Abuse (0.03421606441378519)\n",
      "Feature 11: Driver At Fault (0.03059462871459191)\n",
      "Feature 12: Driver Distracted By (0.024411611302001646)\n"
     ]
    }
   ],
   "source": [
    "importance = rf.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]  # Sort indices in descending order\n",
    "for i, feature_index in enumerate(indices):\n",
    "    print(f\"Feature {i+1}: {x_numeric.columns[feature_index]} ({importance[feature_index]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Injury         84376\n",
       "Minor Injury      19981\n",
       "Serious Injury     1022\n",
       "Name: Injury Severity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to the nature of traffic accidents, \n",
    "# the class attribute is significantly imbalanced as you can see from the output below.\n",
    "df[\"Injury Severity\"].value_counts()\n",
    "\n",
    "# This is why balance class attribute is needed in the next code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated class distribution:\n",
      "[83894 20148  1337]\n"
     ]
    }
   ],
   "source": [
    "# balance class attribute data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# separate class attribute out before balance\n",
    "y=df[\"Injury Severity\"]\n",
    "x=df.drop([\"Injury Severity\"],axis=1)\n",
    "nrow=df.shape[0]\n",
    "ncol=df.shape[1]\n",
    "\n",
    "\n",
    "class_distribution = df[\"Injury Severity\"].value_counts().to_list()\n",
    "\n",
    "total= sum(class_distribution)\n",
    "\n",
    "class_weights = {0: class_distribution[0]/total,\n",
    "                 1: class_distribution[1]/total,\n",
    "                 2: class_distribution[2]/total}\n",
    "\n",
    "# before conducting ROSE or SMOTE, we need to create a synthetic classification dataset with controlled characteristics. \n",
    "x, y = make_classification(\n",
    "    n_samples=nrow,  # the number of rows in clean dataset\n",
    "    n_features=ncol-1,  # Total number of features excluding the class attribute\n",
    "    n_informative=ncol-1,  # Number of informative features in your dataset\n",
    "    n_redundant=0,  # Number of redundant features \n",
    "    n_repeated=0,  # Number of repeated features \n",
    "    n_classes=3,  # Number of classes in class attribute\n",
    "    weights=class_weights,  # Class distribution of the target variable\n",
    "    random_state=42)\n",
    "\n",
    "print(\"Generated class distribution:\")\n",
    "print(np.bincount(y))\n",
    "\n",
    "# if you need to split data into 20% test set and 80% training set.\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE:\n",
      "[83894 20148  1337]\n",
      "After SMOTE:\n",
      "[83894 83894 83894]\n"
     ]
    }
   ],
   "source": [
    "# SMOTE(Synthetic Minority Over-sampling Technique)\n",
    "# this is a popular technique used to address class imbalance in classification  \n",
    "# it's designed to handle the minority class is underrepresented compared to the majority class.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_s_resampled, y_s_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "print(np.bincount(y))\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(np.bincount(y_s_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation: Stratified K-fold Cross-Validation\n",
    "# Similar to K-fold, but it ensures that each fold maintains the same class distribution as the original dataset.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "k=10\n",
    "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in stratified_kfold.split(x_s_resampled, y_s_resampled):\n",
    "    # Obtain the training and testing sets for this fold\n",
    "    x_train, x_test = x_s_resampled[train_index], x_s_resampled[test_index]\n",
    "    y_train, y_test = y_s_resampled[train_index], y_s_resampled[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6612762237762237\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# for training model, there are 5 algorithms have been selected: logistic regression, Naive bayes, KNN, \n",
    "# decision tree and gradient boosting algorithm\n",
    "\n",
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model=LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver=\"newton-cg\",\n",
    "    warm_start=True).fit(x_train,y_train)\n",
    "y_pred=lr_model.predict(x_test)\n",
    "lr_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6569055944055944\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model=GaussianNB().fit(x_train,y_train)\n",
    "y_pred=nb_model.predict(x_test)\n",
    "nb_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", nb_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "# TD: After the initial result, the Naive Bayes and Logistic Regression \n",
    "# Seems under - fitting. We will attempt to enhance the performance before \n",
    "# the final report step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9034885568976478\n",
      "F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model=KNeighborsClassifier(n_neighbors=500,n_jobs=8).fit(x_train,y_train)\n",
    "y_pred=knn_model.predict(x_test)\n",
    "knn_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", knn_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "#TD: After the initial result, we found out the KNN model seems over fitting.\n",
    "# This issue will be handled before the final report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203671328671329\n",
      "F1 Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ct_model=DecisionTreeClassifier(max_depth=10,random_state=42).fit(x_train,y_train)\n",
    "y_pred=ct_model.predict(x_test)\n",
    "ct_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", ct_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "# This works perfectly so far. Not over or under fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8136919898283534\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gb_model=GradientBoostingClassifier(random_state=42).fit(x_train,y_train)\n",
    "y_pred=gb_model.predict(x_test)\n",
    "gb_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", gb_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "# This was not mentioned during the class. \n",
    "# However, gradient boosting algorithm are used 4 out of 6 journals \n",
    "# from the literature review section. \n",
    "# We decided to use this and the performance seems reasonable so far. \n",
    "# However, this algorithm is quite resource consuming. \n",
    "# Also, it takes a very long time to finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis of the models \n",
    "\n",
    "# Logistic Regression \n",
    "# It is a statistical algorithm that is used for modelling the relationship between a dependent variable and \n",
    "# independent variables. It estimates the probabilities of the different outcomes using a logistic function\n",
    "# It assumes a linear relationship between features and class attribute.\n",
    "# It can handle both binary and multi-class classification problems.\n",
    "# Strengths:\n",
    "#   Efficient and fast training and prediction.\n",
    "#   Good performance on datasets with linearly separable classes.\n",
    "# Weaknesses:\n",
    "#   Limited in handling complex interactions between features.\n",
    "#   Assumes a linear relationship.\n",
    "\n",
    "# Naive Bayes\n",
    "# It is a simple and straightforward probabilistic algorithm based on Bayes' theorem, which calculates the \n",
    "# conditional probability of a class given independent variables, assuming independence among independent variables. \n",
    "# Strengths:\n",
    "#   Fast training and prediction. \n",
    "#   Handles high-dimensional and sparse data well.\n",
    "#   Good performance with categorical and text data.\n",
    "#   Works well with small training datasets.\n",
    "# Weaknesses:\n",
    "#   Strong assumption of independence between features.\n",
    "#   cannot effectively capture complex interactions or feature dependencies.\n",
    "#   Does not handle missing data inherently.\n",
    "\n",
    "# KNN \n",
    "# It is a non-parametric and instance-based algorithm that used for both classification and regression tasks.\n",
    "# It classifies new instances based on the majority vote of its k nearest neighbors in the feature space.\n",
    "# Can handle both classification and regression tasks.\n",
    "# Strengths:\n",
    "#   Simple and easy to understand and implement.\n",
    "#   Handles multi-class classification naturally.\n",
    "#   Can capture complex decision boundaries.\n",
    "# Weaknesses:\n",
    "#   Computationally expensive during prediction, especially with large datasets.\n",
    "#   Sensitive to the choice of k and the distance metric.\n",
    "\n",
    "# Decision Tree \n",
    "# It is a hierarchical tree-based algorithm that makes decisions or predictions by following a tree-like \n",
    "# structure of conditional rules based on the features of the input data.\n",
    "# Strengths:\n",
    "#   Easy to understand and interpret, especially when visualized\n",
    "#   Can capture non-linear relationships and interactions between features.\n",
    "#   Can handle both categorical and numerical features\n",
    "#   Can handle missing values by making decisions based on available features\n",
    "# Weaknesses:\n",
    "#   Can be prone to over-fitting if not properly controlled.\n",
    "#   Can be sensitive, as small changes in data can result in different trees.\n",
    "\n",
    "# Gradient Boosting Algorithm\n",
    "# It combines multiple weak learners to create a strong predictive model.\n",
    "# It aims to iteratively improve the model's performance by minimizing the errors or residuals of the previous \n",
    "# iterations.\n",
    "# Can handle both regression and classification tasks.\n",
    "# Strengths:\n",
    "#   Excellent predictive performance and accuracy.\n",
    "#   Can capture complex complex relationships and interactions between features.\n",
    "#   Handles a variety of data types and can accommodate missing values.\n",
    "# Weaknesses:\n",
    "#   Computationally expensive and can be time-consuming to train due to the iterative nature.\n",
    "#   Prone to over-fitting if the number of iterations is too high or the base learners are too complex.\n",
    "#   Requires careful parameter tuning and monitoring to prevent over-fitting.\n",
    "#   Less interpretable compared to simple models like logistic regression or decision trees.\n",
    "\n",
    "# overall, all those algorithms are trained with the same dataset and same experimental design,\n",
    "# based on the outputs(accuracy and F1 score), KNN achieves the highest accuracy and F1 score, indicating its \n",
    "# strong performance.\n",
    "# Decision Tree and Gradient Boosting show relatively good performance with accuracy and F1 scores above 0.80.\n",
    "# Logistic Regression and Naive Bayes have lower accuracy and F1 scores compared to the other algorithms, but \n",
    "# they can still provide reasonable results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
