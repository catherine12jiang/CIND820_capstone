{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imbalanced-learn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route Type</th>\n",
       "      <th>Collision Type</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Surface Condition</th>\n",
       "      <th>Light</th>\n",
       "      <th>Traffic Control</th>\n",
       "      <th>Driver Substance Abuse</th>\n",
       "      <th>Driver At Fault</th>\n",
       "      <th>Injury Severity</th>\n",
       "      <th>Driver Distracted By</th>\n",
       "      <th>Speed Limit</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Time of Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>County</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>DETECTED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>15-25</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>County</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>CLOUDY</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Minor Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>15-25</td>\n",
       "      <td>Monday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Municipality</td>\n",
       "      <td>SAME DIR REAR END</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAWN</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>County</td>\n",
       "      <td>SINGLE VEHICLE</td>\n",
       "      <td>CLOUDY</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NONE DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>County</td>\n",
       "      <td>SINGLE VEHICLE</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DRY</td>\n",
       "      <td>DARK LIGHTS ON</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>DETECTED</td>\n",
       "      <td>No</td>\n",
       "      <td>No Injury</td>\n",
       "      <td>NOT DISTRACTED</td>\n",
       "      <td>30-40</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>dawn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Route Type     Collision Type Weather Surface Condition           Light  \\\n",
       "0        County              OTHER   CLEAR               DRY        DAYLIGHT   \n",
       "1        County              OTHER  CLOUDY               DRY        DAYLIGHT   \n",
       "2  Municipality  SAME DIR REAR END   CLEAR               DRY            DAWN   \n",
       "3        County     SINGLE VEHICLE  CLOUDY               DRY        DAYLIGHT   \n",
       "4        County     SINGLE VEHICLE   CLEAR               DRY  DARK LIGHTS ON   \n",
       "\n",
       "  Traffic Control Driver Substance Abuse Driver At Fault Injury Severity  \\\n",
       "0     NO CONTROLS               DETECTED             Yes       No Injury   \n",
       "1     NO CONTROLS          NONE DETECTED             Yes    Minor Injury   \n",
       "2  TRAFFIC SIGNAL          NONE DETECTED              No       No Injury   \n",
       "3     NO CONTROLS          NONE DETECTED              No       No Injury   \n",
       "4     NO CONTROLS               DETECTED              No       No Injury   \n",
       "\n",
       "  Driver Distracted By Speed Limit Day of Week Time of Day  \n",
       "0       NOT DISTRACTED       15-25      Sunday   afternoon  \n",
       "1       NOT DISTRACTED       15-25      Monday     morning  \n",
       "2       NOT DISTRACTED       30-40     Tuesday     morning  \n",
       "3       NOT DISTRACTED       30-40     Tuesday     morning  \n",
       "4       NOT DISTRACTED       30-40    Thursday        dawn  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/processed/cleaned_data.csv\",dtype=\"category\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Injury         84376\n",
       "Minor Injury      19981\n",
       "Serious Injury     1022\n",
       "Name: Injury Severity, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to the nature of traffic accidents, \n",
    "# the class attribute is significantly imbalanced as you can see from the output below.\n",
    "df[\"Injury Severity\"].value_counts()\n",
    "\n",
    "# This is why balance class attribute is needed in the next code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated class distribution:\n",
      "[83894 20148  1337]\n"
     ]
    }
   ],
   "source": [
    "# balance class attribute data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# separate class attribute out before balance\n",
    "y=df[\"Injury Severity\"]\n",
    "x=df.drop([\"Injury Severity\"],axis=1)\n",
    "nrow=df.shape[0]\n",
    "ncol=df.shape[1]\n",
    "\n",
    "\n",
    "class_distribution = df[\"Injury Severity\"].value_counts().to_list()\n",
    "\n",
    "total= sum(class_distribution)\n",
    "\n",
    "class_weights = {0: class_distribution[0]/total,\n",
    "                 1: class_distribution[1]/total,\n",
    "                 2: class_distribution[2]/total}\n",
    "\n",
    "# before conducting ROSE or SMOTE, we need to create a synthetic classification dataset with controlled characteristics. \n",
    "x, y = make_classification(\n",
    "    n_samples=nrow,  # the number of rows in clean dataset\n",
    "    n_features=ncol-1,  # Total number of features excluding the class attribute\n",
    "    n_informative=ncol-1,  # Number of informative features in your dataset\n",
    "    n_redundant=0,  # Number of redundant features \n",
    "    n_repeated=0,  # Number of repeated features \n",
    "    n_classes=3,  # Number of classes in class attribute\n",
    "    weights=class_weights,  # Class distribution of the target variable\n",
    "    random_state=42)\n",
    "\n",
    "print(\"Generated class distribution:\")\n",
    "print(np.bincount(y))\n",
    "\n",
    "# if you need to split data into 20% test set and 80% training set.\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE:\n",
      "[83894 20148  1337]\n",
      "After SMOTE:\n",
      "[83894 83894 83894]\n"
     ]
    }
   ],
   "source": [
    "# SMOTE(Synthetic Minority Over-sampling Technique)\n",
    "# this is a popular technique used to address class imbalance in classification  \n",
    "# it's designed to handle the minority class is underrepresented compared to the majority class.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_s_resampled, y_s_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "print(\"Before SMOTE:\")\n",
    "print(np.bincount(y))\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(np.bincount(y_s_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation: Stratified K-fold Cross-Validation\n",
    "# Similar to K-fold, but it ensures that each fold maintains the same class distribution as the original dataset.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "k=10\n",
    "stratified_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "# Iterate over the folds\n",
    "for train_index, test_index in stratified_kfold.split(x_s_resampled, y_s_resampled):\n",
    "    # Obtain the training and testing sets for this fold\n",
    "    x_train, x_test = x_s_resampled[train_index], x_s_resampled[test_index]\n",
    "    y_train, y_test = y_s_resampled[train_index], y_s_resampled[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 9: Driver Distracted By - Importance: 0.068601184713805\n",
      "Feature 5: Light - Importance: 0.07145062589908156\n",
      "Feature 11: Day of Week - Importance: 0.073614156825297\n",
      "Feature 6: Traffic Control - Importance: 0.07427376158019805\n",
      "Feature 12: Time of Day - Importance: 0.0763123672446986\n",
      "Feature 10: Speed Limit - Importance: 0.07737299323404966\n",
      "Feature 1: Route Type - Importance: 0.07991461127291333\n",
      "Feature 3: Weather - Importance: 0.08113449507280751\n",
      "Feature 7: Driver Substance Abuse - Importance: 0.08249730213612899\n",
      "Feature 2: Collision Type - Importance: 0.08769162477568253\n",
      "Feature 8: Driver At Fault - Importance: 0.0937420022083293\n",
      "Feature 4: Surface Condition - Importance: 0.13339487503700848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=8)\n",
    "rf.fit(x_train, y_train)\n",
    "feature_importance = rf.feature_importances_\n",
    "indices = np.argsort(feature_importance)\n",
    "\n",
    "feature_names = df.drop([\"Injury Severity\"],axis=1).columns\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"Feature {i+1}: {feature_names[i]} - Importance: {feature_importance[i]}\")\n",
    "\n",
    "# the feature Surface Condition - Importance: 0.13339487503700848, contributes most to injury severity   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6620708836617928\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# for training model, there are 5 algorithms have been selected: logistic regression, Naive bayes, KNN, \n",
    "# decision tree and gradient boosting algorithm\n",
    "\n",
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model=LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver=\"newton-cholesky\",\n",
    "    warm_start=True).fit(x_train,y_train)\n",
    "y_pred=lr_model.predict(x_test)\n",
    "lr_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6569055944055944\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model=GaussianNB().fit(x_train,y_train)\n",
    "y_pred=nb_model.predict(x_test)\n",
    "nb_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", nb_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9034885568976478\n",
      "F1 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model=KNeighborsClassifier(n_neighbors=500,n_jobs=8).fit(x_train,y_train)\n",
    "y_pred=knn_model.predict(x_test)\n",
    "knn_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", knn_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "#TD: After the initial result, we found out the KNN model seems over fitting.\n",
    "# This issue will be handled before the final report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7681579783852511\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt_model=DecisionTreeClassifier(max_depth=8,random_state=42).fit(x_train,y_train)\n",
    "y_pred=dt_model.predict(x_test)\n",
    "dt_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", dt_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "feature_names = df.drop([\"Injury Severity\"],axis=1).columns\n",
    "# fig,ax = plt.subplots(figsize=(12, 8))\n",
    "# tree.plot_tree(dt_model, feature_names=feature_names, filled=True, ax=ax)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # This works perfectly so far. Not over or under fitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# import numpy as np\n",
    "# information_gain = mutual_info_classif(x_train, y_train)\n",
    "# # Normalize Information Gain to obtain Gain Ratio\n",
    "# feature_names = df.drop([\"Injury Severity\"],axis=1).columns\n",
    "# sum_information_gain = np.sum(information_gain)\n",
    "# gain_ratio = information_gain / sum_information_gain\n",
    "# indices = np.argsort(information_gain)\n",
    "# print(type(information_gain))\n",
    "\n",
    "# for i in indices:\n",
    "#     print(f\"{feature_names[i]}:\")\n",
    "#     print(f\"   Information Gain: {information_gain[i]}\")\n",
    "#     print(f\"   Gain Ratio: {gain_ratio[i]}\")\n",
    "#     print()\n",
    "\n",
    "# # Information Gain: is a metric used to measure the reduction in entropy or impurity when a specific feature is \n",
    "# # used for splitting the data in a decision tree. A higher information gain indicates that the feature is more \n",
    "# # informative and contributes more to the decision-making process.\n",
    "# # Gain Ratio: is another evaluation metric that takes into account the intrinsic information of the feature, in \n",
    "# # addition to information gain. It helps to handle the bias towards attributes with a large number of distinct \n",
    "# # values. A higher gain ratio suggests that the feature is more valuable for splitting the data\n",
    "\n",
    "# # Surface Condition:\n",
    "# #   Information Gain: 0.13563905191524994\n",
    "# #   Gain Ratio: 0.2898707704019574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8136919898283534\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting algorithm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gb_model=GradientBoostingClassifier(random_state=42).fit(x_train,y_train)\n",
    "y_pred=gb_model.predict(x_test)\n",
    "gb_accuracy=accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Accuracy:\", gb_accuracy)\n",
    "print(\"F1 Score: %.2f\" % f1)\n",
    "\n",
    "# This was not mentioned during the class. \n",
    "# However, gradient boosting algorithm are used 4 out of 6 journals \n",
    "# from the literature review section. \n",
    "# We decided to use this and the performance seems reasonable so far. \n",
    "# However, this algorithm is quite resource consuming. \n",
    "# Also, it takes a very long time to finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis of the models \n",
    "\n",
    "# Logistic Regression \n",
    "# It is a statistical algorithm that is used for modelling the relationship between a dependent variable and \n",
    "# independent variables. It estimates the probabilities of the different outcomes using a logistic function\n",
    "# It assumes a linear relationship between features and class attribute.\n",
    "# It can handle both binary and multi-class classification problems.\n",
    "# Strengths:\n",
    "#   Efficient and fast training and prediction.\n",
    "#   Good performance on datasets with linearly separable classes.\n",
    "# Weaknesses:\n",
    "#   Limited in handling complex interactions between features.\n",
    "#   Assumes a linear relationship.\n",
    "\n",
    "# Naive Bayes\n",
    "# It is a simple and straightforward probabilistic algorithm based on Bayes' theorem, which calculates the \n",
    "# conditional probability of a class given independent variables, assuming independence among independent variables. \n",
    "# Strengths:\n",
    "#   Fast training and prediction. \n",
    "#   Handles high-dimensional and sparse data well.\n",
    "#   Good performance with categorical and text data.\n",
    "#   Works well with small training datasets.\n",
    "# Weaknesses:\n",
    "#   Strong assumption of independence between features.\n",
    "#   cannot effectively capture complex interactions or feature dependencies.\n",
    "#   Does not handle missing data inherently.\n",
    "\n",
    "# KNN \n",
    "# It is a non-parametric and instance-based algorithm that used for both classification and regression tasks.\n",
    "# It classifies new instances based on the majority vote of its k nearest neighbors in the feature space.\n",
    "# Can handle both classification and regression tasks.\n",
    "# Strengths:\n",
    "#   Simple and easy to understand and implement.\n",
    "#   Handles multi-class classification naturally.\n",
    "#   Can capture complex decision boundaries.\n",
    "# Weaknesses:\n",
    "#   Computationally expensive during prediction, especially with large datasets.\n",
    "#   Sensitive to the choice of k and the distance metric.\n",
    "\n",
    "# Decision Tree \n",
    "# It is a hierarchical tree-based algorithm that makes decisions or predictions by following a tree-like \n",
    "# structure of conditional rules based on the features of the input data.\n",
    "# Strengths:\n",
    "#   Easy to understand and interpret, especially when visualized\n",
    "#   Can capture non-linear relationships and interactions between features.\n",
    "#   Can handle both categorical and numerical features\n",
    "#   Can handle missing values by making decisions based on available features\n",
    "# Weaknesses:\n",
    "#   Can be prone to over-fitting if not properly controlled.\n",
    "#   Can be sensitive, as small changes in data can result in different trees.\n",
    "\n",
    "# Gradient Boosting Algorithm\n",
    "# It combines multiple weak learners to create a strong predictive model.\n",
    "# It aims to iteratively improve the model's performance by minimizing the errors or residuals of the previous \n",
    "# iterations.\n",
    "# Can handle both regression and classification tasks.\n",
    "# Strengths:\n",
    "#   Excellent predictive performance and accuracy.\n",
    "#   Can capture complex complex relationships and interactions between features.\n",
    "#   Handles a variety of data types and can accommodate missing values.\n",
    "# Weaknesses:\n",
    "#   Computationally expensive and can be time-consuming to train due to the iterative nature.\n",
    "#   Prone to over-fitting if the number of iterations is too high or the base learners are too complex.\n",
    "#   Requires careful parameter tuning and monitoring to prevent over-fitting.\n",
    "#   Less interpretable compared to simple models like logistic regression or decision trees.\n",
    "\n",
    "# overall, all those algorithms are trained with the same dataset and same experimental design,\n",
    "# based on the outputs(accuracy and F1 score), KNN achieves the highest accuracy and F1 score, indicating its \n",
    "# strong performance.\n",
    "# Decision Tree and Gradient Boosting show relatively good performance with accuracy and F1 scores above 0.80.\n",
    "# Logistic Regression and Naive Bayes have lower accuracy and F1 scores compared to the other algorithms, but \n",
    "# they can still provide reasonable results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
